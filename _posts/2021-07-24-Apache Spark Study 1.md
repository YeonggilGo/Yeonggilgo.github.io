---
layout: post
title: "Spark Study 1. 스파크란"
thumbnail-img: /assets/img/spark.png
tags: [spark]
comments: true
categories: [Spark]
gh-repo: yeonggilgo/spark-practice
---

> [스파크를 다루는 기술]( https://url.kr/m1yhdr )으로 학습한 내용을 기록함.
> 파이썬 언어에 가장 익숙하기 때문에 스칼라 API로 파이썬을 사용하겠음.

# 1장. 스파크 소개
&nbsp;**스파크 : 고속 범용 분산 컴퓨팅 플랫폼**, 맵리듀스보다 10~100배 더 빠른 속도. 스파크는 여러 노드의 분산된 데이터를 활용하지만 사용자가 굳이 알 필요가 없음. 약간의 오버헤드가 발생하기 때문에 단일 머신에서도 처리 할 수 있는 데이터 셋을 다룰 때는 다른 프레임 워크를 사용하자.

&nbsp;스파크의 핵심은 데이터를 디스크가 아니라 메모리에 캐시로 저장하는 인-메모리 실행 모델이며 최대 100배 빠른 성능 향상을 가져다 주었다. 또한 대화형 콘솔 스파크 셸을 이용하여 디버깅 단게에서 컴파일, 배포를 반복하지 않아도 된다.<br>

&nbsp;또한 스파크 클러스터, 하둡 YARN, 아파치 메소스 등 다양한 클러스터 매니저를 사용해 스파크를 실행 할 수 있고 다양한 언어를 지원하여 유연성, 융통성을 갖추고 있다.


## 스파크 컴포넌트에 대한 간략한 소개

#### 1. 코어
&nbsp;다른 컴포넌트에 필요한 기본 기능 제공. 핵심 요소인 **RDD(Resilient Distributed Dataset)**를 제공하는데 분산 데이터 컬렉션을 추상화한 객체로 데이터셋에 적용할 수 있는 연산 및 변환 메서드를
제공한다. 이외에도 네트워킹, 보안, 스케줄링 및 셔플링 등의 기능이 구현되어 있다.
#### 2. 스파크 SQL
&nbsp;스파크와 HiveQL이 지원하는 SQL을 사용해 정형 데이터를 다룰 수 있는 기능을 제공.
#### 3. 스파크 스트리밍
&nbsp;HDFS, 카프카, 플럼, 트위터 등에서 스트리밍 소스를 받아 처리하는 프레임워크. 커스텀 소스도 정의 가능.
#### 4. MLlib
&nbsp;머신 러닝 알고리즘 라이브러리로 의사 결정 트리, 선형 회귀 등 다양한 알고리즘을 지원. RDDSK DataFrame의 데이터셋을 변환하는 머신 러닝 모델 구현.
#### 5. 스파크 GraphX
&nbsp;그래프 RDD(EdgeRDD or VertexRDD)형태의 그래프 구조를 만들 수 있는 기능을 제공. 연결 요소, 최단 경로 탐색 등 중요한 알고리즘이 구현 되어 있음.

## 스파크 프로그램의 실행 과정
1. 로그파일 메모리에 로드
   
   `lines = sc.read.text('hdfs://path/to/the/file')`
   
   로그 파일이 저장 된 블록에서 RAM 메모리로 전송, 저장. 완료 되면 스파크 셸에서 RAM에 저장된 블록(파티션)을 참조 할 수 있으며 이 파티션의 집합이 RDD가 참조하는 분산 컬렉션.
2. RDD를 활용한 작업

   `oom_lines = lines.filter(lines.l.contains("OutOfMemoryError")).cache()`

   `cache`는 추후 다른 잡을 수행 할 때에도 RDD가 메모리에 유지 되도록 할 수 있음.

 본격적인 실습은 2장에서
